{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Word Analyzer\n",
    "\n",
    "Author:  Devin Patel  \n",
    "Purpose: To scrape a selection wikipedia articles and perform sentence analysis on it using TextBlob.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "from bs4 import BeautifulSoup   # For HTML parsing\n",
    "import requests                 # HTTP requests\n",
    "import re                       # Regular expressions\n",
    "import pickle                   # Saving and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Request wikipedia page and return main content\n",
    "# Returns tuple (page_title, {'subsection_title': 'subsection_content'})\n",
    "def wikipage(url):\n",
    "    page_dict = {}\n",
    "    FIRST_SECTION = 'Overview'\n",
    "    \n",
    "    try:\n",
    "    \n",
    "        page = requests.get(url)\n",
    "\n",
    "        # Parse main content portion of page\n",
    "        soop = BeautifulSoup(page.text, 'html')\n",
    "        main_content = soop.find('main', {'id': 'content', 'class': 'mw-body'})\n",
    "        \n",
    "        \n",
    "        # Parse article title\n",
    "        page_title = soop.find('span', {'class': 'mw-page-title-main'}).text\n",
    "        \n",
    "        # Parse subsections\n",
    "        article_contents = main_content.find_all('div', {'class': 'mw-parser-output'})\n",
    "        for content in article_contents:\n",
    "            if not content.find('span', {'typeof': 'mw:File'}):\n",
    "                article_contents = content\n",
    "        \n",
    "        subsections = article_contents.find_all(['h2', 'p'])\n",
    "        \n",
    "        current_section = FIRST_SECTION\n",
    "        \n",
    "        # Read each paragraph, collect them into a dictionary. Stop at Notes section.\n",
    "        for sub in subsections:\n",
    "            # Check if 2nd level header is met. If so, change current section.\n",
    "            if sub.name == 'h2':\n",
    "                if sub.find('span', {'id': 'Notes'}):\n",
    "                    break\n",
    "                \n",
    "                sub_header = sub.find('span', {'class': 'mw-headline'})\n",
    "                current_section = sub_header.text\n",
    "                \n",
    "            \n",
    "            # Check if paragraph is met.\n",
    "            elif sub.name == 'p' and sub.text:\n",
    "                # Remove footnote references using regex\n",
    "                sub_text = re.sub(r'\\[\\d+\\]', '', sub.text).strip()\n",
    "                \n",
    "                # If the paragraph is just whitespace, skip it\n",
    "                if not sub_text: continue\n",
    "                \n",
    "                # Append paragraph to current section\n",
    "                if not current_section in page_dict.keys():\n",
    "                    page_dict[current_section] = sub_text\n",
    "                else:\n",
    "                    page_dict[current_section] += sub_text\n",
    "    \n",
    "    except Exception as e:\n",
    "        return None\n",
    "    # End of for loop\n",
    "    return (page_title, page_dict)\n",
    "# End of wikipage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|████████████████████████████████████████████████████████████████████████████████████████████████████| 100.00%\r"
     ]
    }
   ],
   "source": [
    "# Output status bar\n",
    "def progress_bar(progress, total):\n",
    "    percent = 100 * (progress / float(total))\n",
    "    bar = '█' * int(percent) + '-' * (100 - int(percent))\n",
    "    print(f\"\\r|{bar}| {percent:.2f}%\", end=\"\\r\")\n",
    "\n",
    "\n",
    "# Loop and collect a number of random wikipedia pages\n",
    "COUNT = 100\n",
    "RANDOM_URL = 'https://en.wikipedia.org/wiki/Special:Random'\n",
    "\n",
    "# Will contain tuples (page_title, {'subsection_title': 'subsection_content'})\n",
    "articles = []\n",
    "\n",
    "while len(articles) < COUNT:\n",
    "    page = wikipage(RANDOM_URL)\n",
    "    if page and page[1]:\n",
    "        articles.append(page)\n",
    "    progress_bar(len(articles), COUNT)\n",
    "\n",
    "pickle.dump(articles, open(r'articles.pkl', 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article Title: Annalee Whitmore Fadiman\n",
      "\n",
      "Overview:\n",
      "\tAnnalee Whitmore Fadiman (May 27, 1916 – February 5, 2002) was a scriptwriter for MGM, and World War II foreign correspondent for Life and Time magazines. She was the co-author with Theodore H. White of Thunder Out of China, a book on the Chinese civil war.\n",
      "\n",
      "Early life:\n",
      "\tFadiman was born in Price, Utah, the daughter of bank president Leland Whitmore and Anne Sharp Whitmore, who later became a librarian at New York Public Library. Fadiman graduated from Stanford University in 1937. She was the first woman to be managing editor of the Stanford Daily newspaper. She moved from San Francisco, where she briefly worked at the Agricultural Adjustment Administration, to Los Angeles taking a secretarial pool job at MGM. She wrote several screen treatments including Andy Hardy Meets Debutante (1940) and a screen adaptation for Tish.\n",
      "\n",
      "Career:\n",
      "\tMGM offered her a contract but once the war began, Fadiman found \"the prospect of seven years of Hollywood fluff when the real world was falling apart unendurable,\" and she tried to become a war correspondent but the War Department didn't allow female correspondents.: 141  She became a publicity manager for an aid organization called United China Relief and wrote speeches for Madame Chiang Kai-shek.: 142  During her marriage to correspondent Melville Jacoby, Fadiman survived a month-long escape from the Philippines, and did six weeks of reporting from the front lines of Bataan and Corregidor. Their writings were used nearly unedited, by John Hersey, in his best-seller Men on Bataan.After the death of her husband, she continued to pursue war writing. Theodore H. White persuaded Time Magazine's Henry Luce to petition the War Department for credentials for Fadiman. She became the only female correspondent reporting from Chungking.  She collaborated with White on the best-selling book  Thunder Out of China, about China's role in the war which contained portions of their published dispatches from Time.After the war, Fadiman wrote, lectured, and participated in the radio quiz show Information Please.\n",
      "\n",
      "Personal life:\n",
      "\tShe was married to Melville Jacoby on November 24, 1941 in Manila. He was killed in an airfield accident in Darwin in 1942 after the couple had moved to Brisbane. She married Clifton Fadiman in 1950. The couple had two children, Kim Fadiman and Anne Fadiman. Fadiman lived on Captiva Island, Florida and was a member of the Hemlock Society. She took her own life in 2002 after living with breast cancer and Parkinson's disease.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the contents of a page from wikipage()\n",
    "def print_page(wikipage):\n",
    "    title, page = wikipage[0], wikipage[1]\n",
    "    print(\"Article Title:\", title, end='\\n\\n')\n",
    "    \n",
    "    for sub_title, sub_par in page.items():\n",
    "        print(f\"{sub_title}:\")\n",
    "        print(f\"\\t{sub_par}\\n\")\n",
    "    # End of for loop\n",
    "# End of print_page()\n",
    "\n",
    "# Randomly select an article and print it\n",
    "import random\n",
    "random_article = random.choice(articles)\n",
    "print_page(random_article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import textblob         # For sentence and sentiment analysis\n",
    "import pandas as pd\n",
    "import scipy.io\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exports pyplot figure\n",
    "import os\n",
    "\n",
    "def exportFig(fname):\n",
    "    exportPath = \"images\"\n",
    "\n",
    "    try:\n",
    "        if not os.path.exists(exportPath): os.mkdir(exportPath)\n",
    "    except Exception:\n",
    "        print(\"Can't create a directory to store figures, so they will not be saved.\")\n",
    "        return\n",
    "    \n",
    "    exportPath = os.path.join(exportPath, fname)\n",
    "    \n",
    "    plt.savefig(exportPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\xdp20\\Documents\\Stuff\\Coding Projects\\Wikipedia-Web-Scraper\\wikipedia_analyzer.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xdp20/Documents/Stuff/Coding%20Projects/Wikipedia-Web-Scraper/wikipedia_analyzer.ipynb#X13sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mPolarity\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: textblob\u001b[39m.\u001b[39mTextBlob(x)\u001b[39m.\u001b[39msentiment\u001b[39m.\u001b[39mpolarity)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xdp20/Documents/Stuff/Coding%20Projects/Wikipedia-Web-Scraper/wikipedia_analyzer.ipynb#X13sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# Data is ready\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/xdp20/Documents/Stuff/Coding%20Projects/Wikipedia-Web-Scraper/wikipedia_analyzer.ipynb#X13sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mShape of main dataframe: \u001b[39m\u001b[39m{\u001b[39;00mX_df\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/xdp20/Documents/Stuff/Coding%20Projects/Wikipedia-Web-Scraper/wikipedia_analyzer.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Import Articles\n",
    "articles = pickle.load(open(r'articles.pkl', 'rb'))\n",
    "\n",
    "# Combine all section contents into one string per article\n",
    "for i, article in enumerate(articles):\n",
    "    article_text = \"\"\n",
    "    \n",
    "    for section, content in article[1].items():\n",
    "        article_text += content\n",
    "    articles[i] = (article[0], article_text)\n",
    "    \n",
    "    \n",
    "# Create a dataframe of the articles\n",
    "df = pd.DataFrame(articles, columns=['title', 'content'])\n",
    "\n",
    "\n",
    "# Create a new column for the number of sentences in each article\n",
    "df['sentences'] = df['content'].apply(lambda x: len(textblob.TextBlob(x).sentences))\n",
    "\n",
    "# Create a new column for the number of words in each article\n",
    "df['words'] = df['content'].apply(lambda x: len(textblob.TextBlob(x).words))\n",
    "\n",
    "# Create a new column for the subjectivity of each article\n",
    "df['subjectivity'] = df['content'].apply(lambda x: textblob.TextBlob(x).sentiment.subjectivity)\n",
    "\n",
    "# Create a new column for the sentiment of each article\n",
    "df['Polarity'] = df['content'].apply(lambda x: textblob.TextBlob(x).sentiment.polarity)\n",
    "\n",
    "# Data is ready\n",
    "print(f\"\\nShape of main dataframe: {df.shape}\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscraper-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
